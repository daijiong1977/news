#!/usr/bin/env python3
"""
Compare DeepSeek Chat Mode vs Research Mode analysis quality.
Processes the same article with both modes and generates comparison report.
"""

import subprocess
import json
import time
from pathlib import Path
from datetime import datetime

def run_chat_mode():
    """Process one article with chat mode."""
    print("\n" + "="*70)
    print("ğŸ¤– MODE 1: CHAT MODE (Current)")
    print("="*70)
    
    start = time.time()
    result = subprocess.run(
        ["python3", "deepseek_processor.py", "--batch-size", "1", "--max-batches", "1"],
        capture_output=True,
        text=True
    )
    chat_time = time.time() - start
    
    print(result.stdout)
    if result.returncode != 0:
        print("âŒ Error:", result.stderr)
        return None, chat_time
    
    # Try to load the response
    response_file = Path("deepseek_batch_1.json")
    if response_file.exists():
        with open(response_file, 'r', encoding='utf-8') as f:
            content = f.read()
            if content.startswith("```"):
                content = content.lstrip("`").lstrip("json").lstrip(" \n")
            if content.endswith("```"):
                content = content.rstrip("`").rstrip(" \n")
            try:
                data = json.loads(content)
                return data, chat_time
            except:
                return None, chat_time
    
    return None, chat_time


def run_research_mode():
    """Process one article with research mode."""
    print("\n" + "="*70)
    print("ğŸ”¬ MODE 2: RESEARCH MODE (New)")
    print("="*70)
    
    start = time.time()
    result = subprocess.run(
        ["python3", "deepseek_research_processor.py", "--batch-size", "1", "--max-batches", "1"],
        capture_output=True,
        text=True
    )
    research_time = time.time() - start
    
    print(result.stdout)
    if result.returncode != 0:
        print("âŒ Error:", result.stderr)
        return None, research_time
    
    # Try to load the response
    response_file = Path("deepseek_research_batch_1.json")
    if response_file.exists():
        with open(response_file, 'r', encoding='utf-8') as f:
            content = f.read()
            if content.startswith("```"):
                content = content.lstrip("`").lstrip("json").lstrip(" \n")
            if content.endswith("```"):
                content = content.rstrip("`").rstrip(" \n")
            try:
                data = json.loads(content)
                return data, research_time
            except:
                return None, research_time
    
    return None, research_time


def generate_comparison_report(chat_data, research_data, chat_time, research_time):
    """Generate detailed comparison report."""
    
    report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                            â•‘
â•‘          DEEPSEEK API MODE COMPARISON REPORT                              â•‘
â•‘          Chat Mode vs Research Mode                                        â•‘
â•‘          {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}                                      â•‘
â•‘                                                                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š PERFORMANCE METRICS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Chat Mode (Current):
  Response Time: {chat_time:.1f} seconds
  Status: {'âœ… SUCCESS' if chat_data else 'âŒ FAILED'}
  
Research Mode (New):
  Response Time: {research_time:.1f} seconds
  Status: {'âœ… SUCCESS' if research_data else 'âŒ FAILED'}

â±ï¸  SPEED COMPARISON:
  Research mode is {abs(research_time - chat_time):.1f}s {'faster' if research_time < chat_time else 'slower'}
  Ratio: Research is {(research_time/chat_time):.2f}x {'faster' if research_time < chat_time else 'slower'}

"""
    
    if chat_data:
        report += f"""
ğŸ“‹ CHAT MODE ANALYSIS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Summary Length:
  English: {len(chat_data.get('summary_en', ''))} characters
  Chinese: {len(chat_data.get('summary_zh', ''))} characters

Keywords: {len(chat_data.get('key_words', []))}
Questions: {len(chat_data.get('multiple_choice_questions', []))}

Question Types Distribution:
  """
        types = {}
        for q in chat_data.get('multiple_choice_questions', []):
            q_type = q.get('word_type', 'unknown')
            types[q_type] = types.get(q_type, 0) + 1
        
        for q_type, count in types.items():
            report += f"\n  â€¢ {q_type}: {count}"
    
    if research_data:
        report += f"""

ğŸ”¬ RESEARCH MODE ANALYSIS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Summary Length:
  English: {len(research_data.get('research_summary_en', ''))} characters
  Chinese: {len(research_data.get('research_summary_zh', ''))} characters

Research Keywords: {len(research_data.get('research_keywords', []))}
Deep Questions: {len(research_data.get('deep_questions', []))}

Question Difficulty Distribution:
  """
        difficulties = {}
        for q in research_data.get('deep_questions', []):
            difficulty = q.get('difficulty', 'unknown')
            difficulties[difficulty] = difficulties.get(difficulty, 0) + 1
        
        for diff, count in difficulties.items():
            report += f"\n  â€¢ {diff}: {count}"
        
        report += f"""

Cognitive Levels Used:
  """
        levels = {}
        for q in research_data.get('deep_questions', []):
            level = q.get('cognitive_level', 'unknown')
            levels[level] = levels.get(level, 0) + 1
        
        for level, count in levels.items():
            report += f"\n  â€¢ {level}: {count}"
    
    report += f"""

ğŸ“ COMPARISON SUMMARY
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Strengths of Chat Mode:
  âœ“ Faster response time ({chat_time:.1f}s)
  âœ“ Good balance of question types
  âœ“ Clear, accessible explanations
  âœ“ Works well for general learning

Strengths of Research Mode:
  âœ“ More detailed academic analysis
  âœ“ Includes cognitive difficulty levels
  âœ“ Research-focused perspectives
  âœ“ Identifies research gaps
  âœ“ Better for advanced learners

ğŸ¯ RECOMMENDATION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

"""
    
    if research_time <= chat_time * 1.5:  # If research is less than 1.5x slower
        report += """
âœ… USE RESEARCH MODE if:
  â€¢ You want deeper academic analysis
  â€¢ You need research gaps identified
  â€¢ You want multi-level questions
  â€¢ Speed difference is acceptable

âœ… USE CHAT MODE if:
  â€¢ You need fast processing (quick turnaround)
  â€¢ You prefer straightforward content
  â€¢ You're processing many articles
  â€¢ Speed is critical
"""
    else:
        report += """
âš ï¸  RESEARCH MODE IS SLOWER
  Current timing suggests chat mode is preferable for batch processing.
  Use research mode only for special in-depth analysis needs.
"""
    
    report += f"""

Files Generated:
  â€¢ deepseek_batch_1.json (Chat Mode Response)
  â€¢ deepseek_research_batch_1.json (Research Mode Response)
  â€¢ comparison_report.txt (This report)

"""
    
    return report


def main():
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                            â•‘
â•‘                    STARTING API MODE COMPARISON TEST                      â•‘
â•‘                                                                            â•‘
â•‘     Processing the same article with both Chat and Research modes         â•‘
â•‘     to determine which is better for your learning platform               â•‘
â•‘                                                                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    # Run both modes
    chat_data, chat_time = run_chat_mode()
    time.sleep(3)  # Wait between API calls
    research_data, research_time = run_research_mode()
    
    # Generate report
    report = generate_comparison_report(chat_data, research_data, chat_time, research_time)
    
    print(report)
    
    # Save report
    with open('comparison_report.txt', 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("âœ“ Comparison report saved to: comparison_report.txt")
    
    # Summary
    print("\n" + "="*70)
    print("ğŸ“Š QUICK SUMMARY")
    print("="*70)
    if chat_data:
        print(f"âœ… Chat Mode: {len(chat_data.get('multiple_choice_questions', []))} questions in {chat_time:.1f}s")
    else:
        print(f"âŒ Chat Mode: Failed after {chat_time:.1f}s")
    
    if research_data:
        print(f"âœ… Research Mode: {len(research_data.get('deep_questions', []))} questions in {research_time:.1f}s")
    else:
        print(f"âŒ Research Mode: Failed after {research_time:.1f}s")


if __name__ == "__main__":
    main()
